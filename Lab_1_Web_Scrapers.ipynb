{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab 1: Simple Web Scraping with Requests\n",
    "\n",
    "Objective: Learn how to parse and retrieve data from a website in python.\n",
    "\n",
    "Successful Outcome: Successfully access a piece of data, save it to a variable, and print it out.\n",
    "\n",
    "Remember to shut down your server when you are done by clicking Control Panel -> Shut Down Server "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Preliminaries\n",
    "\n",
    "This is where we import the needed modules and \"get\" the web page we want to get data from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are importing the requests and BeautifulSoup modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## Requests is the module that actually goes out and accesses the website.\n",
    "## BeautifulSoup helps with formatting and parsing the html.\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Nineteen_Eighty-Four\")\n",
    "\n",
    "\n",
    "## Here we are accessing wikipedia using the requests module, running this block will \n",
    "## give us a snapshot of the page at the time of access.\n",
    "\n",
    "## When you run this block there should be no output, go ahead, give it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Reading the Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now that we have grabbed a snapshot of the page, let's see what happens when we print it out!\n",
    "print(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As you can see, printing the page just gives us a response code, when what we want is the page content.\n",
    "## This can be simply accomplished by accessing the .content of the page object\n",
    "## we created earlier and printing it out.\n",
    "print(page.content)\n",
    "\n",
    "## This command will give you all of the page's content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As you can see we have the page content now, but its not that easy to read, that's why Beautiful Soup is very useful.\n",
    "## Now we are going to access and parse the content as html using beautiful soup's html.parser.\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "## Let's see what the parser got us!\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Parsing the HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are not that familiar with what HTML is here is a link to an article that will explain HTML much better than I could.\n",
    "\n",
    "https://www.w3schools.com/html/html_intro.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As you can see the HTML is now much easier to read, and with some simple commands we can search for objects within the html very easily.\n",
    "## The find_all command is very useful in finding all instances of the text or class you give it.\n",
    "## Text parameters go into the first argument, class in the second.\n",
    "## For this notebook we are going to find the heading of the wiki article. \n",
    "## We know the class we are looking for is 'firstheading', you can see this in the output above.\n",
    "## Google chromes inspect element is very useful for finding the object that you're looking for as well.\n",
    "body = soup.find_all('', class_='firstHeading')\n",
    "\n",
    "## Let's see what the soup found.\n",
    "print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## So its easy as that. All we have is the HTML line, so some trimming is necessary. If you want just the title \n",
    "## you would omit the rest of the line using python's string commands. (string[startIndex:EndIndex])\n",
    "## First we have to cast it as a string though.\n",
    "title = str(body)\n",
    "title = title[57:77]\n",
    "print(title)\n",
    "\n",
    "## 57 and 77 are the start and end positions of the title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there it is! With this simple script, a forloop, and a file with valid links, you could grab as many \n",
    "titles from wikipedia as you want. By changing the parameters of the find function around, other pieces of data can be grabbed as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Step 4: More Complex Scraping\n",
    "\n",
    "Now let's try something a bit more complex. Here is a website that just has example data tables. Let's learn how to grab all the information from a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These first lines are just like we saw earlier. Getting the webpage, and getting a soup object of the page content.\n",
    "## For easy to read formatting and parsing.\n",
    "page = requests.get(\"https://datatables.net/examples/basic_init/zero_configuration.html\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "\n",
    "## The complex part is working with tables, which are a very common and relevant piece of data\n",
    "## on a webpage. The only thing you have to know is that a table is made of 'tr' and 'td' objects.\n",
    "## A 'tr' object is a row and 'td' is all of the data in that row. (tableRow) and (tableData).\n",
    "\n",
    "\n",
    "## So here we are simply running a for loop that gets the first and only 'tr' object and all of it's corresponding\n",
    "## 'td' objects.\n",
    "data = [[cell.get_text(strip=True) for cell in row.find_all('td')] for row in soup.find_all(\"tr\")]\n",
    "\n",
    "## Now let's print out all of the row data for the table on the page!\n",
    "for entry in data:\n",
    "    print(str(entry))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we got every single table entry in the example table. With some string manipulation we can extract each entry very easily to get the name, age, and other variables. Now let's trim and save that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here we are creating a txtfile called \"scraper.txt\" in your outputs/ folder.\n",
    "import os\n",
    "txtfile = open(os.path.join(os.path.expanduser('~'), \"outputs/Lab1\", \"scraper.txt\"),\"w\")\n",
    "\n",
    "## Here we are writing the header to the file.\n",
    "txtfile.write(\"Name, Position, Age, Start_date, Salary\")\n",
    "\n",
    "##Here we are taking each entry and trimming the undesirable characters and writing it to the file\n",
    "for entry in data:\n",
    "    text = str(entry)\n",
    "    text = text.replace(']', '')\n",
    "    text = text.replace('[', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "    print(text)\n",
    "    txtfile.write(text + \"\\n\")\n",
    "    \n",
    "txtfile.close()\n",
    "\n",
    "## Go check the file out in the outputs/ folder, it's named scraper.txt!\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Congratulations!\n",
    "You have learned how to successfully scrape simple and complex structures from webpages!\n",
    "If you want to train your scraping muscles a little bit more, ponder this exercise.\n",
    "\n",
    "https://www.immobiliare.it/Roma/agenzie_immobiliari_provincia-Roma.html\n",
    "Go to this website and grab/scrape 5 different fields and save them to a .txt file. This will help you identify how different objects on a webpage require different steps to grab them in their entirety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
